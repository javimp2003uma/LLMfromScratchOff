{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f244f0f-f368-42a9-bfe8-0dc4b37ab500",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run GPTarchitecture.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa1912e5-4e09-4792-8698-0b84dcf91fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ed0001-777d-48ba-bec9-4fb37990b214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "# import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d55b42c-3ef5-46a5-976a-2906c3207b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the text generation loss: cross-entropy and perplexity\n",
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e7ceb0-d55e-46f8-a9e1-6d0967e8ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b119f4-a26e-4aa6-8e16-a86b10feabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1deb4801-83a4-448f-a0db-0ee06da484cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04f2031-af86-411f-ba3f-1a5a8bc1836b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df2c6de5-83a2-4552-97f0-ee7a9f3e911b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a7746d-4a1d-4bda-a521-83f4a3bc8d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"The goal is to get the average log probability as close to 0 as possible by\n",
    "updating the model's weights as part of the training process\"\"\"\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63a436c2-3023-450d-a71b-700a9298a13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca490bb9-fb85-4ae2-aa75-51cfa828bd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits shape:\", logits.shape)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98a2e475-2e49-4bd4-ba5c-0b06efc83994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e76e3cd5-ceaa-47a4-8ccc-80e0785e5143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f3c48f-4610-4baf-a906-687f7f0e03cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5dbf2bf-d0ac-4e64-985c-24d03c687fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess text files\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"A2H0H0R1/Animal-nutrition\")\n",
    "\n",
    "text_data = \"\"\n",
    "\n",
    "# Iterate through the first 100 samples\n",
    "for sample in ds['train'][:100]['text']:\n",
    "    # Strip leading and trailing whitespace\n",
    "    cleaned_text = sample.strip()\n",
    "    # Check if text is not empty\n",
    "    if cleaned_text:\n",
    "        # Add cleaned text and a newline for separation\n",
    "        text_data += cleaned_text  # Two newlines for separation\n",
    "text_data = text_data.replace(\"Question: \", \"\").replace(\"Answer: \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9087d49b-8ff1-46a5-ba1f-86350fb291fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Uncomment the following code block in order to execute the train with Animal nutrition dataset\\nimport os\\n\\ndef combine_texts(text_dir):\\n    combined_text = \"\"\\n    files_processed = 0\\n    for filename in os.listdir(text_dir):\\n        if filename.endswith(\\'.txt\\'):\\n            with open(os.path.join(text_dir, filename), \\'r\\', encoding=\\'utf-8\\') as file:\\n                combined_text += file.read() + \"\\n\"  # Add a newline to separate contents of different files\\n                files_processed += 1\\n                if files_processed >= 20:\\n                    break\\n\\n    return combined_text\\n\\ntext_data = combine_texts(\"openwebtext\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Uncomment the following code block in order to execute the train with Animal nutrition dataset\n",
    "import os\n",
    "\n",
    "def combine_texts(text_dir):\n",
    "    combined_text = \"\"\n",
    "    files_processed = 0\n",
    "    for filename in os.listdir(text_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(text_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                combined_text += file.read() + \"\\n\"  # Add a newline to separate contents of different files\n",
    "                files_processed += 1\n",
    "                if files_processed >= 20:\n",
    "                    break\n",
    "\n",
    "    return combined_text\n",
    "\n",
    "text_data = combine_texts(\"openwebtext\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05d787f3-b68c-4ce5-b7a2-506332fa192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 100390\n",
      "Tokens: 19364\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d2044bc-9c12-438b-b79b-b11232d55444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63d74704-2071-47b5-972b-3c05da1375f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataloader from chapter 2 ...\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeb483e5-5c4e-4981-815e-2ad82c114aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c827564-d2b4-4a79-8144-0a3eaf9a8ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c864c5cc-f965-48f2-8e3c-e91698b4ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0819d5dc-bf13-4837-ab13-e9bf6a9b2429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.955262530933727\n",
      "Validation loss: 10.996375799179077\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)         # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "torch.manual_seed(123)   # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad():    # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7633a5e7-89b5-41dc-9a90-e14b6f3b1060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training the model ...\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93776372-9422-4316-a486-43a12b4cd732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.481, Val loss 9.925\n",
      "Ep 1 (Step 000005): Train loss 7.759, Val loss 8.432\n",
      "Ep 1 (Step 000010): Train loss 6.649, Val loss 7.494\n",
      "Ep 1 (Step 000015): Train loss 5.997, Val loss 7.200\n",
      "Ep 1 (Step 000020): Train loss 5.739, Val loss 6.999\n",
      "Ep 1 (Step 000025): Train loss 5.527, Val loss 6.921\n",
      "Ep 1 (Step 000030): Train loss 5.263, Val loss 6.655\n",
      "Why is water important for animals and plants?                        The the food, and animals, and animals, and animals, and the food, and the food, and animals, and the\n",
      "Ep 2 (Step 000035): Train loss 5.165, Val loss 6.559\n",
      "Ep 2 (Step 000040): Train loss 4.775, Val loss 6.368\n",
      "Ep 2 (Step 000045): Train loss 4.332, Val loss 6.241\n",
      "Ep 2 (Step 000050): Train loss 4.315, Val loss 6.120\n",
      "Ep 2 (Step 000055): Train loss 3.874, Val loss 5.898\n",
      "Ep 2 (Step 000060): Train loss 4.177, Val loss 5.915\n",
      "Ep 2 (Step 000065): Train loss 3.962, Val loss 5.811\n",
      "Why is water important for animals and plants?                                                  \n",
      "Ep 3 (Step 000070): Train loss 3.890, Val loss 5.795\n",
      "Ep 3 (Step 000075): Train loss 3.705, Val loss 5.782\n",
      "Ep 3 (Step 000080): Train loss 3.436, Val loss 5.771\n",
      "Ep 3 (Step 000085): Train loss 3.274, Val loss 5.649\n",
      "Ep 3 (Step 000090): Train loss 2.825, Val loss 5.502\n",
      "Ep 3 (Step 000095): Train loss 2.984, Val loss 5.426\n",
      "Why is water important for animals and plants?                                                  \n",
      "Ep 4 (Step 000100): Train loss 2.955, Val loss 5.349\n",
      "Ep 4 (Step 000105): Train loss 2.638, Val loss 5.425\n",
      "Ep 4 (Step 000110): Train loss 2.497, Val loss 5.378\n",
      "Ep 4 (Step 000115): Train loss 2.409, Val loss 5.421\n",
      "Ep 4 (Step 000120): Train loss 2.367, Val loss 5.301\n",
      "Ep 4 (Step 000125): Train loss 2.238, Val loss 5.323\n",
      "Ep 4 (Step 000130): Train loss 1.999, Val loss 5.336\n",
      "Why is water important for animals and plants?    * The water content of foods can vary greatly, and animals.          * The water content of foods can vary greatly, and animals.   * The water content of foods can be used to\n",
      "Ep 5 (Step 000135): Train loss 2.192, Val loss 5.257\n",
      "Ep 5 (Step 000140): Train loss 1.968, Val loss 5.312\n",
      "Ep 5 (Step 000145): Train loss 1.792, Val loss 5.343\n",
      "Ep 5 (Step 000150): Train loss 1.823, Val loss 5.356\n",
      "Ep 5 (Step 000155): Train loss 2.053, Val loss 5.302\n",
      "Ep 5 (Step 000160): Train loss 1.819, Val loss 5.313\n",
      "Why is water important for animals and plants?                                                  \n",
      "Ep 6 (Step 000165): Train loss 1.707, Val loss 5.322\n",
      "Ep 6 (Step 000170): Train loss 1.575, Val loss 5.414\n",
      "Ep 6 (Step 000175): Train loss 1.331, Val loss 5.364\n",
      "Ep 6 (Step 000180): Train loss 1.604, Val loss 5.257\n",
      "Ep 6 (Step 000185): Train loss 1.565, Val loss 5.274\n",
      "Ep 6 (Step 000190): Train loss 1.515, Val loss 5.305\n",
      "Ep 6 (Step 000195): Train loss 1.311, Val loss 5.357\n",
      "Why is water important for animals and plants?                                                  \n",
      "Ep 7 (Step 000200): Train loss 1.121, Val loss 5.447\n",
      "Ep 7 (Step 000205): Train loss 1.167, Val loss 5.481\n",
      "Ep 7 (Step 000210): Train loss 1.037, Val loss 5.530\n",
      "Ep 7 (Step 000215): Train loss 0.999, Val loss 5.490\n",
      "Ep 7 (Step 000220): Train loss 0.916, Val loss 5.388\n",
      "Ep 7 (Step 000225): Train loss 0.850, Val loss 5.437\n",
      "Ep 7 (Step 000230): Train loss 0.801, Val loss 5.380\n",
      "Why is water important for animals and plants?   * In animals, water content of water is a diet play a critical role in the stage of the water content of foods, and metabolic processes, and they require. This leads to an important to a continuous extraction with petroleum ether for a more\n",
      "Ep 8 (Step 000235): Train loss 0.741, Val loss 5.474\n",
      "Ep 8 (Step 000240): Train loss 0.679, Val loss 5.511\n",
      "Ep 8 (Step 000245): Train loss 0.667, Val loss 5.492\n",
      "Ep 8 (Step 000250): Train loss 0.736, Val loss 5.558\n",
      "Ep 8 (Step 000255): Train loss 0.634, Val loss 5.508\n",
      "Ep 8 (Step 000260): Train loss 0.627, Val loss 5.668\n",
      "Why is water important for animals and plants? The water content of a variety of food. On the overall nutritional value of a diet. It is related to the overall composition of the food. The water content of the nutrient content of the nutrient content of the food.What are the nutrient content\n",
      "Ep 9 (Step 000265): Train loss 0.563, Val loss 5.671\n",
      "Ep 9 (Step 000270): Train loss 0.483, Val loss 5.748\n",
      "Ep 9 (Step 000275): Train loss 0.472, Val loss 5.693\n",
      "Ep 9 (Step 000280): Train loss 0.452, Val loss 5.712\n",
      "Ep 9 (Step 000285): Train loss 0.499, Val loss 5.706\n",
      "Ep 9 (Step 000290): Train loss 0.406, Val loss 5.736\n",
      "Ep 9 (Step 000295): Train loss 0.441, Val loss 5.747\n",
      "Why is water important for animals and plants? The different animal species.How have modern analytical methods evolved to address the overall nutritional value of nutrients are the efficient breakdown of different types of their potential supply of nutrients to the nutrient composition of different foods, the high specific heat of water, the absence\n",
      "Ep 10 (Step 000300): Train loss 0.361, Val loss 5.772\n",
      "Ep 10 (Step 000305): Train loss 0.316, Val loss 5.807\n",
      "Ep 10 (Step 000310): Train loss 0.335, Val loss 5.788\n",
      "Ep 10 (Step 000315): Train loss 0.281, Val loss 5.810\n",
      "Ep 10 (Step 000320): Train loss 0.201, Val loss 5.854\n",
      "Ep 10 (Step 000325): Train loss 0.206, Val loss 5.861\n",
      "Why is water important for animals and plants? The difference between dry matter composition of water from as a dry matter basis is that the diet allows for a more valid comparison of nutrient content because it eliminates the nutrient content of foods can vary greatly, ranging from the food. Additionally, the various classes\n",
      "Ep 11 (Step 000330): Train loss 0.240, Val loss 5.958\n",
      "Ep 11 (Step 000335): Train loss 0.200, Val loss 5.957\n",
      "Ep 11 (Step 000340): Train loss 0.208, Val loss 5.940\n",
      "Ep 11 (Step 000345): Train loss 0.209, Val loss 5.845\n",
      "Ep 11 (Step 000350): Train loss 0.221, Val loss 5.937\n",
      "Ep 11 (Step 000355): Train loss 0.164, Val loss 5.926\n",
      "Ep 11 (Step 000360): Train loss 0.180, Val loss 5.929\n",
      "Why is water important for animals and plants? The water, and how do the general metabolism of foods can greatly impact the overall nutritional value of foods is crucial for a dry matter basis, and water content of water, and animal-based foods, which can lead to a range of plant nutrition\n",
      "Ep 12 (Step 000365): Train loss 0.159, Val loss 5.985\n",
      "Ep 12 (Step 000370): Train loss 0.172, Val loss 5.971\n",
      "Ep 12 (Step 000375): Train loss 0.201, Val loss 6.050\n",
      "Ep 12 (Step 000380): Train loss 0.145, Val loss 6.089\n",
      "Ep 12 (Step 000385): Train loss 0.147, Val loss 6.104\n",
      "Ep 12 (Step 000390): Train loss 0.177, Val loss 6.016\n",
      "Ep 12 (Step 000395): Train loss 0.110, Val loss 6.057\n",
      "Why is water important for animals and plants? The water is necessary for a variety of plant and animal products. The water content of growing plants is related to the stage of nutrient content of foods is important roles in younger plants compared to an important aspect of the body temperature. The water content of\n",
      "Ep 13 (Step 000400): Train loss 0.104, Val loss 6.182\n",
      "Ep 13 (Step 000405): Train loss 0.110, Val loss 6.193\n",
      "Ep 13 (Step 000410): Train loss 0.116, Val loss 6.128\n",
      "Ep 13 (Step 000415): Train loss 0.102, Val loss 6.168\n",
      "Ep 13 (Step 000420): Train loss 0.089, Val loss 6.092\n",
      "Ep 13 (Step 000425): Train loss 0.072, Val loss 6.117\n",
      "Why is water important for animals and plants? The different foods, plants, while the nitrogen-free extractives fraction is a mixture of all components not determined in the other fractions.What are the limitations of the proximate analysis of foods in terms of foods, and an underestimation of foods\n",
      "Ep 14 (Step 000430): Train loss 0.076, Val loss 6.180\n",
      "Ep 14 (Step 000435): Train loss 0.060, Val loss 6.205\n",
      "Ep 14 (Step 000440): Train loss 0.070, Val loss 6.247\n",
      "Ep 14 (Step 000445): Train loss 0.078, Val loss 6.370\n",
      "Ep 14 (Step 000450): Train loss 0.063, Val loss 6.241\n",
      "Ep 14 (Step 000455): Train loss 0.056, Val loss 6.298\n",
      "Ep 14 (Step 000460): Train loss 0.039, Val loss 6.259\n",
      "Why is water important for animals and plants? The difference between expressing food. Crude protein is calculated from the nitrogen content of foods is a balance of the food, determined by a modification of a balance of the animal body varies with age, while the food, and a range from a defined\n",
      "Ep 15 (Step 000465): Train loss 0.055, Val loss 6.235\n",
      "Ep 15 (Step 000470): Train loss 0.046, Val loss 6.276\n",
      "Ep 15 (Step 000475): Train loss 0.065, Val loss 6.283\n",
      "Ep 15 (Step 000480): Train loss 0.045, Val loss 6.305\n",
      "Ep 15 (Step 000485): Train loss 0.045, Val loss 6.272\n",
      "Ep 15 (Step 000490): Train loss 0.055, Val loss 6.272\n",
      "Why is water important for animals and plants? The difference between dry matter composition of food production systems?  Understanding the dry matter composition of foods is crucial for developing more efficient and sustainable food production systems. Dry matter refers to the total amount of solids in a food sample after removing moisture\n",
      "Ep 16 (Step 000495): Train loss 0.045, Val loss 6.291\n",
      "Ep 16 (Step 000500): Train loss 0.033, Val loss 6.332\n",
      "Ep 16 (Step 000505): Train loss 0.041, Val loss 6.375\n",
      "Ep 16 (Step 000510): Train loss 0.039, Val loss 6.404\n",
      "Ep 16 (Step 000515): Train loss 0.037, Val loss 6.407\n",
      "Ep 16 (Step 000520): Train loss 0.037, Val loss 6.390\n",
      "Ep 16 (Step 000525): Train loss 0.035, Val loss 6.392\n",
      "Why is water important for animals and plants? The difference between dry matter composition of foods can vary greatly, the dry matter composition of foods can be used to develop new food products with improved nutritional profiles. By understanding the nutrient composition of different foods, food scientists can design new products that are more\n",
      "Ep 17 (Step 000530): Train loss 0.060, Val loss 6.375\n",
      "Ep 17 (Step 000535): Train loss 0.052, Val loss 6.384\n",
      "Ep 17 (Step 000540): Train loss 0.024, Val loss 6.396\n",
      "Ep 17 (Step 000545): Train loss 0.036, Val loss 6.436\n",
      "Ep 17 (Step 000550): Train loss 0.032, Val loss 6.457\n",
      "Ep 17 (Step 000555): Train loss 0.040, Val loss 6.422\n",
      "Ep 17 (Step 000560): Train loss 0.030, Val loss 6.408\n",
      "Why is water important for animals and plants?  The water, and intestines, crude protein, and ether extract. Modern analytical methods are being developed to characterize foods is determined by animals have a balance of food to a constant weight at 100°C. Ash content of growing plants is higher\n",
      "Ep 18 (Step 000565): Train loss 0.025, Val loss 6.430\n",
      "Ep 18 (Step 000570): Train loss 0.023, Val loss 6.426\n",
      "Ep 18 (Step 000575): Train loss 0.018, Val loss 6.358\n",
      "Ep 18 (Step 000580): Train loss 0.021, Val loss 6.411\n",
      "Ep 18 (Step 000585): Train loss 0.024, Val loss 6.501\n",
      "Ep 18 (Step 000590): Train loss 0.020, Val loss 6.594\n",
      "Why is water important for animals and plants? The difference, and how do the general metabolism of cells. Other organic acids, insulate organs, and play a role in hormone production and monitor changes in hormone production and regulate body temperature. Therefore, dry matter, carbohydrates, lipids, proteins\n",
      "Ep 19 (Step 000595): Train loss 0.012, Val loss 6.506\n",
      "Ep 19 (Step 000600): Train loss 0.014, Val loss 6.408\n",
      "Ep 19 (Step 000605): Train loss 0.013, Val loss 6.428\n",
      "Ep 19 (Step 000610): Train loss 0.021, Val loss 6.455\n",
      "Ep 19 (Step 000615): Train loss 0.017, Val loss 6.493\n",
      "Ep 19 (Step 000620): Train loss 0.014, Val loss 6.446\n",
      "Ep 19 (Step 000625): Train loss 0.018, Val loss 6.496\n",
      "Why is water important for animals and plants? The difference between dry matter composition of water from as little as 60 g/kg in concentrates to the water that an animal consumes through drinking. Both types of foods is often expressed on a dry matter basis. This allows for a more valid comparison\n",
      "Ep 20 (Step 000630): Train loss 0.016, Val loss 6.507\n",
      "Ep 20 (Step 000635): Train loss 0.012, Val loss 6.513\n",
      "Ep 20 (Step 000640): Train loss 0.024, Val loss 6.541\n",
      "Ep 20 (Step 000645): Train loss 0.015, Val loss 6.505\n",
      "Ep 20 (Step 000650): Train loss 0.015, Val loss 6.523\n",
      "Ep 20 (Step 000655): Train loss 0.016, Val loss 6.495\n",
      "Why is water important for animals and plants? The different animal species. This energy is often expressed on a dry matter basis to account for the water content of foods can tailor their water content of growing plants higher in younger plants compared to older ones?  Water content of growing plants is higher\n",
      "Training completed in 1.22 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train the LLM using the training function defined above\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Why is water important for animals and plants?\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "execution_time_minutes = (time.time() - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ebd3ae2-fd41-4b36-a2fd-e48a65420962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d25fffc-e1fd-4e3a-a35e-8a5d0a047ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Why is water important for animals and plants?\n",
      "\n",
      "The chemical reactions water meat organic nutrients. On the overall nutritional value of nutrients in water can also help identify biomarkers\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Why is water important for animals and plants?\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k = 25,\n",
    "    temperature = 1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0458ea6-f4bd-465b-abe8-a5083e717678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncheckpoint = torch.load(\"GPTtrained.pth\")\\nmodel = GPTModel(GPT_CONFIG_124M)\\nmodel.load_state_dict(checkpoint[\"model_state_dict\"])\\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\\nmodel.train();\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"GPTtrained.pth\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "checkpoint = torch.load(\"GPTtrained.pth\")\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train();\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvLLM",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
